##################################################################
### This script converts bed files of islet ATAC-seq data      ###
### to a input format for neural network training              ###
### It can be used to create input from the 2x2=4 input        ###
### datasets (different filtering in Kundaje pipeline), which  ###
### were originally derived from identical ATAC-seq reads.     ###
##################################################################

##### load torch modules & dependencies
module load torch/20170221-k80-gcc5.4.0
module load python/2.7.10-gcc4.9.3
export BEDTOOLS=/apps/well/bedtools/2.24.0/
export PATH=$BEDTOOLS:$PATH
export BASSETDIR=/well/got2d/agata/Basset/
export PATH=$BASSETDIR/src:$PATH
export PYTHONPATH=$BASSETDIR/src:$PYTHONPATH
export LUA_PATH="$BASSETDIR/src/?.lua;$LUA_PATH"
export PATH=${PATH}:/well/got2d/agata/bin/weblogo/
export PATH=${PATH}:/apps/well/meme/4.11.2_2/bin

#preprocess Original features, no preliminary steps needed
preprocess_features.py -y -m 400 -s 1000 -n -o neg_original_islets -c /well/got2d/agata/Basset/data/genomes/human.hg19.genome neg_original_samples.txt
bedtools getfasta -fi /well/got2d/agata/Basset/data/genomes/hg19.fa -bed neg_original_islets.bed -s -fo neg_original_islets.fa


#For 1CPM, gunzip files first:
gunzip *

#Then rename 1CPM files to bed format:
for FILE in *filtered; do mv "$FILE" $(echo "$FILE" | sed 's/filtered/filtered.bed/'); done

#preprocess 1CPM features
preprocess_features.py -y -m 400 -s 1000 -n -o 1CPM_islets -c /well/got2d/agata/Basset/data/genomes/human.hg19.genome 1CPM_samples.txt
bedtools getfasta -fi /well/got2d/agata/Basset/data/genomes/hg19.fa -bed 1CPM_islets.bed -s -fo 1CPM_islets.fa

###################################

# Add zeros again in R, open R first
#test <- read.table("1CPM_islets_act.txt")
#test$Neg <- rep(0, nrow(test))
#write.table(test, file = "n1CPM_islets_act_test.txt", row.names = TRUE, col.names = TRUE, quote = FALSE, sep = "\t")


### 2x2 options: Test and validation creation by random permutation (all chromosomes)
# vs chr1-chr2 and Kundaje filtering: original vs 1CPM ###

### in the original directory, make two directories: chr1-chr2 and random ###
cd /well/mccarthy/users/maxlouis/oxford2/CNN_project/preprocessing/original/
mkdir chr1chr2
mkdir random

### hold out chr1 and chr2 - for test and validation sets ###
#go to directory for chr1chr2
cd chr1chr2

CHR1=`awk '{print $1}' ../neg_original_islets.bed  | sort | uniq -c | grep -w chr1 | awk '{printf $1}'`
CHR2=`awk '{print $1}' ../neg_original_islets.bed  | sort | uniq -c | grep -w chr2 | awk '{printf $1}'`

### BED file
grep -vw chr1 ../neg_original_islets.bed | grep -vw chr2 > chr1_2_last.bed
grep -w chr2 ../neg_original_islets.bed >> chr1_2_last.bed
grep -w chr1 ../neg_original_islets.bed >> chr1_2_last.bed

### act file
grep -vw chr1 ../neg_original_islets_act_test.txt | grep -vw chr2 > chr1_2_last.act.txt
grep -w chr2 ../neg_original_islets_act_test.txt >> chr1_2_last.act.txt
grep -w chr1 ../neg_original_islets_act_test.txt >> chr1_2_last.act.txt

#Make the final training file
bedtools getfasta -fi /well/got2d/agata/Basset/data/genomes/hg19.fa -bed chr1_2_last.bed -s -fo original_islets.chr1_2.fa
seq_hdf5.permute.py -c -v $CHR2 -t $CHR1 original_islets.chr1_2.fa chr1_2_last.act.txt original_islets.chr1_2.h5

#Resulting numbers:
#232953 training sequences
#24075 test sequences
#24104 validation sequences

##################  random permute on original ##################
#go to directory for random
cd random

#Do random permute, use same numbers as original_chr1chr2 to maintain ratio:
#232953 training sequences
#24075 test sequences
#24104 validation sequences
seq_hdf5.py -c -r -t 24075 -v 24104 ../neg_original_islets.fa ../neg_original_islets_act_test.txt original_islets.h5


################## 1CPM preprocessing ##################

### in the 1CPM directory, make two directories: chr1-chr2 and random ###
cd /well/mccarthy/users/maxlouis/oxford2/CNN_project/preprocessing/1CPM
mkdir chr1chr2
mkdir random

### hold out chr1 and chr2 - for test and validation sets ###
# go to directory for chr1chr2
cd /well/mccarthy/users/maxlouis/oxford2/CNN_project/preprocessing/1CPM/chr1chr2

CHR1=`awk '{print $1}' ../1CPM_islets.bed  | sort | uniq -c | grep -w chr1 | awk '{printf $1}'`
CHR2=`awk '{print $1}' ../1CPM_islets.bed  | sort | uniq -c | grep -w chr2 | awk '{printf $1}'`

### BED file
grep -vw chr1 ../1CPM_islets.bed | grep -vw chr2 > chr1_2_last.bed
grep -w chr2 ../1CPM_islets.bed >> chr1_2_last.bed
grep -w chr1 ../1CPM_islets.bed >> chr1_2_last.bed

### act file
grep -vw chr1 ../n1CPM_islets_act_test.txt | grep -vw chr2 > chr1_2_last.act.txt
grep -w chr2 ../n1CPM_islets_act_test.txt >> chr1_2_last.act.txt
grep -w chr1 ../n1CPM_islets_act_test.txt >> chr1_2_last.act.txt

#Make the final training file
bedtools getfasta -fi /well/got2d/agata/Basset/data/genomes/hg19.fa -bed chr1_2_last.bed -s -fo 1CPM_islets.chr1_2.fa
seq_hdf5.permute.py -c -v $CHR2 -t $CHR1 1CPM_islets.chr1_2.fa chr1_2_last.act.txt 1CPM_islets.chr1_2.h5

# Resulting numbers:
# 104324 training sequences
# 11053 test sequences
# 10737 validation sequences

# Neg Resulting numbers:
# 190519 training sequences
# 19889 test sequences
# 19487 validation sequences




##################  random permute on 1CPM ##################
#go to directory for random
cd /well/mccarthy/users/maxlouis/oxford2/CNN_project/preprocessing/1CPM/random

# Do random permute
# 104324 training sequences
# 11053 test sequences
# 10737 validation sequences
seq_hdf5.py -c -r -t 19889 -v 19487 ../1CPM_islets.fa ../n1CPM_islets_act_test.txt 1CPM_islets.h5

### Preprocessing finished. You can now start training CNN with these four
### .h5-files. See next script.

##################  remove negative column random permute on 1CPM ##################
# Go to R to remove column
#test <- read.table("1CPM_islets_act.txt")
#test <- test[,-9]
#test$Neg <- rep(0, nrow(test))
#write.table(test, file = "8colneg1CPM_islets_act.txt", row.names = TRUE, col.names = TRUE, quote = FALSE, sep = "\t")
# go to directory for random
cd /well/mccarthy/users/maxlouis/oxford2/CNN_project/preprocessing/1CPM/real_random

# Do random permute
# 104324 training sequences
# 19889 test sequences
# 19487 validation sequences
seq_hdf5.py -c -r -t 19889 -v 19487 ../1CPM_islets.fa ../8colneg1CPM_islets_act.txt 1CPM_islets.h5

### Preprocessing finished. You can now start training CNN with these four
### .h5-files. See next script.
