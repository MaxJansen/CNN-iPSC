##################################################################
### This script converts bed files of islet ATAC-seq data      ###
### to a input format for neural network training              ###
### It can be used to create input from the 2x2=4 input        ###
### datasets (different filtering in Kundaje pipeline), which  ###
### were originally derived from identical ATAC-seq reads.     ###
##################################################################

##### load torch modules & dependencies
module load torch/20170221-k80-gcc5.4.0
module load python/2.7.10-gcc4.9.3
export BEDTOOLS=/apps/well/bedtools/2.24.0/
export PATH=$BEDTOOLS:$PATH
export BASSETDIR=/well/got2d/agata/Basset/
export PATH=$BASSETDIR/src:$PATH
export PYTHONPATH=$BASSETDIR/src:$PYTHONPATH
export LUA_PATH="$BASSETDIR/src/?.lua;$LUA_PATH"
export PATH=${PATH}:/well/got2d/agata/bin/weblogo/
export PATH=${PATH}:/apps/well/meme/4.11.2_2/bin

# The directory should contain:
# The 8 .bed-files for the 8 stages (as received from Marta)
# a list of samples: 1CPM_samples.txt
# The negative dataset training sequences you just created in previous steps:
# neg_1CPM_zeros_shuf.bed

##################  random permute on 1CPM ##################
#go to directory for random
cd /well/mccarthy/users/maxlouis/oxford2/CNN_project/preprocessing/ \
negative_set/data/final_step/1CPM

#For 1CPM, gunzip files first:
gunzip *

#Then rename 1CPM files to bed format:
for FILE in *filtered; do mv "$FILE" $(echo "$FILE" | sed 's/filtered/filtered.bed/'); done

#preprocess 1CPM features, do this with islet data from 8 stages AND the negative data
preprocess_features.py -y -m 400 -s 1000 -n -o 1CPM_islets -c /well/got2d/agata/Basset/data/genomes/human.hg19.genome 1CPM_samples.txt
bedtools getfasta -fi /well/got2d/agata/Basset/data/genomes/hg19.fa -bed 1CPM_islets.bed -s -fo 1CPM_islets.fa

###################################
# Add zeros in R
./make_col_0.R

# Make directories for the two test and validation set creation approaches:
mkdir chr1chr2
mkdir random

### hold out chr1 and chr2 - for test and validation sets ###
# go to directory for chr1chr2
cd /well/mccarthy/users/maxlouis/oxford2/CNN_project/preprocessing/1CPM/chr1chr2

CHR1=`awk '{print $1}' ../1CPM_islets.bed  | sort | uniq -c | grep -w chr1 | awk '{printf $1}'`
CHR2=`awk '{print $1}' ../1CPM_islets.bed  | sort | uniq -c | grep -w chr2 | awk '{printf $1}'`

### BED file
grep -vw chr1 ../1CPM_islets.bed | grep -vw chr2 > chr1_2_last.bed
grep -w chr2 ../1CPM_islets.bed >> chr1_2_last.bed
grep -w chr1 ../1CPM_islets.bed >> chr1_2_last.bed

### act file
grep -vw chr1 ../n1CPM_islets_act_test.txt | grep -vw chr2 > chr1_2_last.act.txt
grep -w chr2 ../n1CPM_islets_act_test.txt >> chr1_2_last.act.txt
grep -w chr1 ../n1CPM_islets_act_test.txt >> chr1_2_last.act.txt

# Make the final training file
bedtools getfasta -fi /well/got2d/agata/Basset/data/genomes/hg19.fa -bed chr1_2_last.bed -s -fo 1CPM_islets.chr1_2.fa
seq_hdf5.permute.py -c -v $CHR2 -t $CHR1 1CPM_islets.chr1_2.fa chr1_2_last.act.txt 1CPM_islets.chr1_2.h5

# Neg Resulting numbers:
# 190519 training sequences
# 19889 test sequences
# 19487 validation sequences

##################  random permute on 1CPM ##################


##################  remove negative column and random permute on 1CPM ##########
# Rscript to remove neg column:
./remove_neg_col.R
#go to directory for random
cd /well/mccarthy/users/maxlouis/oxford2/CNN_project/preprocessing/ \
negative_set/data/final_step/1CPM/real_random

# Do random permute, use the same numbers as chr1chr2
# 190519 training sequences
# 19889 test sequences
# 19487 validation sequences
seq_hdf5.py -c -r -t 19889 -v 19487 ../1CPM_islets.fa ../8colneg1CPM_islets_act.txt 1CPM_islets.h5

### Preprocessing finished. You can now start training CNN with these four
### .h5-files. See next script.
